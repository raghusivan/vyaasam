Yes, Confluence supports Wiki Markup (deprecated in the new editor) and Markdown for formatting. However, the Cloud version mainly relies on rich text editing with macros.

Since you want a markup version for easy pasting, below is a Markdown-style version that should work well when pasted into Confluence Cloud:


---

🔹 Elasticsearch Message Processing Job - Technical Design

> This document outlines the technical approach for extracting Message IDs from Elasticsearch, tracking them in MySQL, batching them for processing, and uploading the results to S3.




---

📌 Job Execution Workflow

Step 1: Extract Message IDs from Elasticsearch

Query Elasticsearch (ES) index for messages in a given date range.

Retrieve only the message_id to optimize performance.

Store message IDs in MySQL (message_tracking table) for tracking.


Step 2: Store Messages in MySQL for Tracking

Each message_id is stored in message_tracking.

Tracking includes:

Batch ID

Processing status (Pending, In Progress, Completed, Failed)

Start Date

S3 Path (populated after a successful upload)



Step 3: Process Messages in Batches

Messages are batched (e.g., 500 messages per batch) and processed in parallel.

For each message:

Query ES for message, history, status, audit.

Create a JSON file.

Upload JSON to S3.

Update MySQL tracking table.



Step 4: Job Execution & Tracking

Check the job_runs table for pending jobs.

If no pending jobs exist, read from the config file to determine start_date.

Record each job execution and prepare for the next scheduled run.



---

🗄️ Database Schema

Table: job_runs (Tracks Job Execution)

CREATE TABLE job_runs (
    job_id INT AUTO_INCREMENT PRIMARY KEY,
    start_date DATE NOT NULL,
    end_date DATE DEFAULT NULL,
    status ENUM('running', 'completed', 'failed') DEFAULT 'running',
    next_run_date DATE DEFAULT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

Table: message_tracking (Tracks Messages per Job)

CREATE TABLE message_tracking (
    message_id VARCHAR(255) PRIMARY KEY,
    batch_id INT NOT NULL,
    status ENUM('pending', 'in_progress', 'completed', 'failed') DEFAULT 'pending',
    start_date DATE NOT NULL,
    s3_path VARCHAR(500) DEFAULT NULL,
    last_attempted TIMESTAMP DEFAULT NULL
);


---

⚙️ Processing Options

Option 1: AWS Lambda + SQS (Serverless)

🔹 How It Works

SQS Queue stores message_ids.

Lambda instances process messages:

Query ES, generate JSON, upload to S3, and update MySQL.



✅ Pros
✔️ Auto-scales with AWS Lambda.
✔️ No EC2 instance management.

❌ Cons
❌ Limited execution time (15 min max).
❌ Requires Step Functions for longer runs.


---

Option 2: EC2 Instance with Python Multiprocessing

🔹 How It Works

A Python script on EC2 retrieves batches from MySQL.

Multiprocessing is used to process messages in parallel.

Each worker:

Fetches data from ES.

Generates JSON.

Uploads to S3.

Updates MySQL.



✅ Pros
✔️ More control over execution.
✔️ No Lambda cold start latency.

❌ Cons
❌ Limited scalability without Auto Scaling.
❌ Requires monitoring and maintenance.


---

Option 3: Auto-Scaling EC2 Instances Behind ALB

🔹 How It Works

Multiple EC2 instances run behind an Application Load Balancer (ALB).

Each instance processes batches independently.


✅ Pros
✔️ Highly scalable.
✔️ Better fault tolerance.

❌ Cons
❌ Higher AWS cost.
❌ Requires ALB setup and instance monitoring.


---

🛠️ Technical Implementation

📌 Extracting Message IDs from Elasticsearch

from elasticsearch import Elasticsearch
import mysql.connector

es = Elasticsearch("http://your-elasticsearch-host")

def extract_message_ids(start_date, end_date):
    """Fetch message IDs from Elasticsearch and store them in MySQL"""
    query = {"query": {"range": {"timestamp": {"gte": start_date, "lte": end_date}}}}
    results = es.search(index="messages", body=query, size=10000)

    message_ids = [hit["_id"] for hit in results["hits"]["hits"]]

    conn = mysql.connector.connect(host="your-db", user="user", password="pass", database="your_db")
    cursor = conn.cursor()

    batch_id = int(time.time())
    insert_query = "INSERT INTO message_tracking (message_id, batch_id, status, start_date) VALUES (%s, %s, 'pending', %s)"
    cursor.executemany(insert_query, [(msg, batch_id, start_date) for msg in message_ids])

    conn.commit()
    cursor.close()
    conn.close()


---

📌 Processing Batches Using Multiprocessing (EC2)

import multiprocessing
import boto3
import json

s3 = boto3.client('s3')

def process_message(message_id):
    """Process a single message: fetch from ES, generate JSON, upload to S3"""
    message_data = es.get(index="messages", id=message_id)["_source"]

    file_path = f"/tmp/{message_id}.json"
    with open(file_path, "w") as f:
        json.dump(message_data, f)

    s3_key = f"messages/{message_id}.json"
    s3.upload_file(file_path, "your-bucket", s3_key)

    return s3_key

def process_batch(batch):
    """Process a batch of messages in parallel"""
    with multiprocessing.Pool(processes=4) as pool:
        results = pool.map(process_message, batch)

    return results


---

📌 Automating Job Execution with Systemd

sudo nano /etc/systemd/system/message_job.service

[Unit]
Description=Message Processing Job
After=network.target

[Service]
ExecStart=/usr/bin/python3 /home/ubuntu/job.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target

sudo systemctl enable message_job
sudo systemctl start message_job


---

📌 Instructions for Confluence Formatting

1. Use the "Code Snippet" Macro for SQL/Python/Bash scripts.


2. Use the "Table" Macro for structured data like job_runs and message_tracking.


3. Use the "Status" Macro for tracking job statuses (Pending, Completed, Failed).


4. Use "Bullet Lists" for Pros/Cons and options.


5. Use the "Expand" Macro for detailed sections (optional).




---

🚀 Conclusion

The job extracts message_ids from Elasticsearch, stores them in MySQL, and processes them in batches.

Three deployment options: AWS Lambda + SQS, EC2 multiprocessing, Auto-Scaling EC2 with ALB.

Job tracking ensures fault tolerance and prevents duplicate processing.

Systemd automates execution on EC2.


🎯 This ensures a fault-tolerant, scalable, and efficient batch processing system! 🚀


---

📌 How to Paste in Confluence Cloud

1. Go to your Confluence Cloud page.


2. Click "Create" → "Blank Page".


3. Paste the above content (it will auto-format).


4. Use "Code Snippet" for Python/SQL.


5. Publish the page.



🚀 Now ready for Confluence! 🚀

