Here is your Confluence Cloud-ready document with macro placeholders (/code, /table, /status, /divider, /expand, /warning, /info, etc.). You can copy and paste this directly into Confluence and replace the macro placeholders with actual formatting while editing.


---

ğŸ”¹ Elasticsearch Message Processing Job - Technical Design

/info
This document outlines the technical design for a batch job that extracts Message IDs from Elasticsearch, tracks them in MySQL, processes them in batches, generates JSON files, and uploads them to S3. The system ensures job tracking, fault tolerance, and scalability across different processing architectures.

/divider

ğŸ“Œ Job Execution Workflow

Step 1: Extract Message IDs from Elasticsearch

âœ”ï¸ Query Elasticsearch (ES) index for messages in a given date range.
âœ”ï¸ Retrieve only the message_id to optimize performance.
âœ”ï¸ Store message IDs in MySQL (message_tracking table) for tracking.

Step 2: Store Messages in MySQL for Tracking

âœ”ï¸ Each message_id is stored in message_tracking.
âœ”ï¸ Tracking includes:

Batch ID

Processing status â†’ /status (Pending, In Progress, Completed, Failed)

Start Date

S3 Path (populated after a successful upload)


Step 3: Process Messages in Batches

âœ”ï¸ Messages are batched (e.g., 500 messages per batch) and processed in parallel.
âœ”ï¸ For each message:

Query ES for message, history, status, audit.

Create a JSON file.

Upload JSON to S3.

Update MySQL tracking table.


Step 4: Job Execution & Tracking

âœ”ï¸ Check the job_runs table for pending jobs before starting a new run.
âœ”ï¸ If no pending jobs exist, read from the config file to determine start_date.
âœ”ï¸ Record each job execution and prepare for the next scheduled run.

/divider

ğŸ—„ï¸ Database Schema

ğŸ“Œ Use the "/table" macro below to format properly in Confluence.

Table: job_runs (Tracks Job Execution)

/table

Table: message_tracking (Tracks Messages per Job)

/table

/divider

âš™ï¸ Processing Options

ğŸ“Œ Use the "/expand" macro to collapse these sections in Confluence for better readability.

/expand

Option 1: AWS Lambda + SQS (Serverless)

ğŸ”¹ How It Works

SQS Queue stores message_ids.

Lambda instances process messages:

Query ES, generate JSON, upload to S3, and update MySQL.



âœ… Pros
âœ”ï¸ Auto-scales with AWS Lambda.
âœ”ï¸ No EC2 instance management.

âŒ Cons
âŒ Limited execution time (15 min max).
âŒ Requires Step Functions for longer runs.
/expand

/expand

Option 2: EC2 Instance with Python Multiprocessing

ğŸ”¹ How It Works

A Python script on EC2 retrieves batches from MySQL.

Multiprocessing is used to process messages in parallel.

Each worker:

Fetches data from ES.

Generates JSON.

Uploads to S3.

Updates MySQL.



âœ… Pros
âœ”ï¸ More control over execution.
âœ”ï¸ No Lambda cold start latency.

âŒ Cons
âŒ Limited scalability without Auto Scaling.
âŒ Requires monitoring and maintenance.
/expand

/expand

Option 3: Auto-Scaling EC2 Instances Behind ALB

ğŸ”¹ How It Works

Multiple EC2 instances run behind an Application Load Balancer (ALB).

Each instance processes batches independently.


âœ… Pros
âœ”ï¸ Highly scalable.
âœ”ï¸ Better fault tolerance.

âŒ Cons
âŒ Higher AWS cost.
âŒ Requires ALB setup and instance monitoring.
/expand

/divider

ğŸ› ï¸ Technical Implementation

ğŸ“Œ Extracting Message IDs from Elasticsearch

ğŸ“Œ Use the "/code" macro for Python code snippets.

/code python
from elasticsearch import Elasticsearch
import mysql.connector

es = Elasticsearch("http://your-elasticsearch-host")

def extract_message_ids(start_date, end_date):
"""Fetch message IDs from Elasticsearch and store them in MySQL"""
query = {"query": {"range": {"timestamp": {"gte": start_date, "lte": end_date}}}}
results = es.search(index="messages", body=query, size=10000)

message_ids = [hit["_id"] for hit in results["hits"]["hits"]]  

conn = mysql.connector.connect(host="your-db", user="user", password="pass", database="your_db")  
cursor = conn.cursor()  

batch_id = int(time.time())  
insert_query = "INSERT INTO message_tracking (message_id, batch_id, status, start_date) VALUES (%s, %s, 'pending', %s)"  
cursor.executemany(insert_query, [(msg, batch_id, start_date) for msg in message_ids])  

conn.commit()  
cursor.close()  
conn.close()

/code

/divider

ğŸ“Œ Confluence Formatting Tips

âœ”ï¸ Use the "/table" macro for structured data.
âœ”ï¸ Use the "/code" macro for Python/SQL snippets.
âœ”ï¸ Use the "/expand" macro to collapse long sections.
âœ”ï¸ Use "/status" for tracking job execution (Pending, Completed, Failed).
âœ”ï¸ Use "/divider" for clear section breaks.
âœ”ï¸ Use "/info", "/warning", and "/error" for important notes.

/divider

ğŸš€ Conclusion

âœ”ï¸ The job extracts message_ids from Elasticsearch, stores them in MySQL, and processes them in batches.
âœ”ï¸ Three deployment options: AWS Lambda + SQS, EC2 multiprocessing, Auto-Scaling EC2 with ALB.
âœ”ï¸ Job tracking ensures fault tolerance and prevents duplicate processing.
âœ”ï¸ Systemd automates execution on EC2 for stability.

ğŸ¯ This ensures a fault-tolerant, scalable, and efficient batch processing system! ğŸš€

/divider

ğŸ“Œ How to Paste in Confluence Cloud

1ï¸âƒ£ Go to your Confluence Cloud page.
2ï¸âƒ£ Click â€œCreateâ€ â†’ "Blank Page".
3ï¸âƒ£ Paste this content directly (retain macro placeholders).
4ï¸âƒ£ Replace macro placeholders (/table, /code, /status, etc.).
5ï¸âƒ£ Click "Publish".

ğŸš€ Now your Confluence page is ready with proper formatting! ğŸš€

