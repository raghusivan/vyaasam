Here is your Confluence Cloud-ready document with macro placeholders (/code, /table, /status, /divider, /expand, /warning, /info, etc.). You can copy and paste this directly into Confluence and replace the macro placeholders with actual formatting while editing.


---

🔹 Elasticsearch Message Processing Job - Technical Design

/info
This document outlines the technical design for a batch job that extracts Message IDs from Elasticsearch, tracks them in MySQL, processes them in batches, generates JSON files, and uploads them to S3. The system ensures job tracking, fault tolerance, and scalability across different processing architectures.

/divider

📌 Job Execution Workflow

Step 1: Extract Message IDs from Elasticsearch

✔️ Query Elasticsearch (ES) index for messages in a given date range.
✔️ Retrieve only the message_id to optimize performance.
✔️ Store message IDs in MySQL (message_tracking table) for tracking.

Step 2: Store Messages in MySQL for Tracking

✔️ Each message_id is stored in message_tracking.
✔️ Tracking includes:

Batch ID

Processing status → /status (Pending, In Progress, Completed, Failed)

Start Date

S3 Path (populated after a successful upload)


Step 3: Process Messages in Batches

✔️ Messages are batched (e.g., 500 messages per batch) and processed in parallel.
✔️ For each message:

Query ES for message, history, status, audit.

Create a JSON file.

Upload JSON to S3.

Update MySQL tracking table.


Step 4: Job Execution & Tracking

✔️ Check the job_runs table for pending jobs before starting a new run.
✔️ If no pending jobs exist, read from the config file to determine start_date.
✔️ Record each job execution and prepare for the next scheduled run.

/divider

🗄️ Database Schema

📌 Use the "/table" macro below to format properly in Confluence.

Table: job_runs (Tracks Job Execution)

/table

Table: message_tracking (Tracks Messages per Job)

/table

/divider

⚙️ Processing Options

📌 Use the "/expand" macro to collapse these sections in Confluence for better readability.

/expand

Option 1: AWS Lambda + SQS (Serverless)

🔹 How It Works

SQS Queue stores message_ids.

Lambda instances process messages:

Query ES, generate JSON, upload to S3, and update MySQL.



✅ Pros
✔️ Auto-scales with AWS Lambda.
✔️ No EC2 instance management.

❌ Cons
❌ Limited execution time (15 min max).
❌ Requires Step Functions for longer runs.
/expand

/expand

Option 2: EC2 Instance with Python Multiprocessing

🔹 How It Works

A Python script on EC2 retrieves batches from MySQL.

Multiprocessing is used to process messages in parallel.

Each worker:

Fetches data from ES.

Generates JSON.

Uploads to S3.

Updates MySQL.



✅ Pros
✔️ More control over execution.
✔️ No Lambda cold start latency.

❌ Cons
❌ Limited scalability without Auto Scaling.
❌ Requires monitoring and maintenance.
/expand

/expand

Option 3: Auto-Scaling EC2 Instances Behind ALB

🔹 How It Works

Multiple EC2 instances run behind an Application Load Balancer (ALB).

Each instance processes batches independently.


✅ Pros
✔️ Highly scalable.
✔️ Better fault tolerance.

❌ Cons
❌ Higher AWS cost.
❌ Requires ALB setup and instance monitoring.
/expand

/divider

🛠️ Technical Implementation

📌 Extracting Message IDs from Elasticsearch

📌 Use the "/code" macro for Python code snippets.

/code python
from elasticsearch import Elasticsearch
import mysql.connector

es = Elasticsearch("http://your-elasticsearch-host")

def extract_message_ids(start_date, end_date):
"""Fetch message IDs from Elasticsearch and store them in MySQL"""
query = {"query": {"range": {"timestamp": {"gte": start_date, "lte": end_date}}}}
results = es.search(index="messages", body=query, size=10000)

message_ids = [hit["_id"] for hit in results["hits"]["hits"]]  

conn = mysql.connector.connect(host="your-db", user="user", password="pass", database="your_db")  
cursor = conn.cursor()  

batch_id = int(time.time())  
insert_query = "INSERT INTO message_tracking (message_id, batch_id, status, start_date) VALUES (%s, %s, 'pending', %s)"  
cursor.executemany(insert_query, [(msg, batch_id, start_date) for msg in message_ids])  

conn.commit()  
cursor.close()  
conn.close()

/code

/divider

📌 Confluence Formatting Tips

✔️ Use the "/table" macro for structured data.
✔️ Use the "/code" macro for Python/SQL snippets.
✔️ Use the "/expand" macro to collapse long sections.
✔️ Use "/status" for tracking job execution (Pending, Completed, Failed).
✔️ Use "/divider" for clear section breaks.
✔️ Use "/info", "/warning", and "/error" for important notes.

/divider

🚀 Conclusion

✔️ The job extracts message_ids from Elasticsearch, stores them in MySQL, and processes them in batches.
✔️ Three deployment options: AWS Lambda + SQS, EC2 multiprocessing, Auto-Scaling EC2 with ALB.
✔️ Job tracking ensures fault tolerance and prevents duplicate processing.
✔️ Systemd automates execution on EC2 for stability.

🎯 This ensures a fault-tolerant, scalable, and efficient batch processing system! 🚀

/divider

📌 How to Paste in Confluence Cloud

1️⃣ Go to your Confluence Cloud page.
2️⃣ Click “Create” → "Blank Page".
3️⃣ Paste this content directly (retain macro placeholders).
4️⃣ Replace macro placeholders (/table, /code, /status, etc.).
5️⃣ Click "Publish".

🚀 Now your Confluence page is ready with proper formatting! 🚀

