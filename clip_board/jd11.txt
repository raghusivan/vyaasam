# Directory structure:
# - src/
#     - config_loader.py
#     - alert_service.py
#     - sqs_client.py
#     - s3_uploader.py
#     - logger.py
# - main/
#     - lambda_handler.py
# - config/
#     - config.ini
# - maintenance/ (for logs, temp files, etc.)

# src/logger.py
import logging

logger = logging.getLogger("alert_lambda")
logger.setLevel(logging.INFO)

handler = logging.StreamHandler()
formatter = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


# src/config_loader.py
import os
import configparser

config = configparser.ConfigParser()
config.read('config/config.ini')

REGION = config['aws']['region']
QUEUE_URL = config['aws']['queue_url']
CERT_PATH = config['cert']['path']
BASE_URL = config['shield']['base_url']
DEFAULT_DELAY = int(config['lambda']['default_delay'])

ACCESS_TOKEN = os.getenv("ACCESS_TOKEN")
BUCKET_NAME = os.getenv("BUCKET_NAME")

PROXIES = {
    "http": os.getenv("HTTP_PROXY"),
    "https": os.getenv("HTTPS_PROXY")
} if os.getenv("HTTP_PROXY") else None


# src/alert_service.py
import json
import requests
from src.config_loader import ACCESS_TOKEN, BASE_URL, CERT_PATH, PROXIES
from src.logger import logger

def start_alert_job(start_date, end_date):
    url = f"{BASE_URL}/data-extract/v1/alerts"
    headers = {
        "Authorization": f"Bearer {ACCESS_TOKEN}",
        "Content-Type": "application/json"
    }
    payload = {
        "startDate": start_date,
        "endDate": end_date
    }
    logger.info(f"Starting alert job with payload: {payload}")
    response = requests.post(url, headers=headers, json=payload, verify=CERT_PATH, proxies=PROXIES)
    response.raise_for_status()
    job_id = response.json().get("jobId")
    logger.info(f"Started job_id: {job_id}")
    return job_id

def check_job_status(job_id):
    url = f"{BASE_URL}/data-extract/v1/alerts/{job_id}"
    headers = {"Authorization": f"Bearer {ACCESS_TOKEN}"}
    logger.info(f"Checking job status for job_id: {job_id}")
    return requests.get(url, headers=headers, verify=CERT_PATH, proxies=PROXIES)

def download_alert_data(job_id):
    response = check_job_status(job_id)
    logger.info(f"Received status code {response.status_code} for job_id {job_id}")
    if response.status_code == 200:
        data = response.json()
        with open("maintenance/response.json", "w") as f:
            json.dump(data, f)
        logger.info(f"Job {job_id} completed. Data written to response.json")
        return True
    elif response.status_code == 202:
        logger.info(f"Job {job_id} still in progress.")
        return False
    else:
        logger.error(f"Unexpected status code: {response.status_code}")
        raise Exception(f"Unexpected status code: {response.status_code}")


# src/sqs_client.py
import boto3
from src.config_loader import REGION, QUEUE_URL
from src.logger import logger

sqs = boto3.client("sqs", region_name=REGION)

def send_job_to_sqs(job_id, delay_seconds):
    logger.info(f"Sending job_id '{job_id}' to SQS with delay {delay_seconds}s")
    response = sqs.send_message(
        QueueUrl=QUEUE_URL,
        MessageBody=job_id,
        DelaySeconds=delay_seconds
    )
    logger.info(f"SQS send_message response: {response}")
    return response


# src/s3_uploader.py
import boto3
from src.config_loader import BUCKET_NAME
from src.logger import logger

def upload_to_s3(file_path, object_key):
    logger.info(f"Uploading file {file_path} to s3://{BUCKET_NAME}/{object_key}")
    s3 = boto3.client("s3")
    s3.upload_file(file_path, BUCKET_NAME, object_key)
    logger.info("Upload completed successfully.")


# main/lambda_handler.py
import json
import traceback
from src.alert_service import start_alert_job, download_alert_data
from src.sqs_client import send_job_to_sqs
from src.s3_uploader import upload_to_s3
from src.config_loader import DEFAULT_DELAY, config
from src.logger import logger

def lambda_handler(event, context):
    logger.info("Lambda invocation started.")
    logger.info(f"Event received: {json.dumps(event)}")

    try:
        if 'Records' in event and event['Records'][0].get('eventSource') == 'aws:sqs':
            for record in event['Records']:
                job_id = record.get('body')
                logger.info(f"Processing job_id from SQS: {job_id}")
                try:
                    completed = download_alert_data(job_id)
                    if completed:
                        upload_to_s3("maintenance/response.json", f"alerts/{job_id}.json")
                    else:
                        delay = int(event.get('delaySeconds', DEFAULT_DELAY))
                        send_job_to_sqs(job_id, delay)
                except Exception as err:
                    logger.exception(f"Error processing job_id {job_id}: {err}")
        else:
            start_date = event.get('startDate') or config['shield']['start_date']
            end_date = event.get('endDate') or config['shield']['end_date']
            delay = int(event.get('delaySeconds', DEFAULT_DELAY))
            job_id = start_alert_job(start_date, end_date)
            send_job_to_sqs(job_id, delay)
            return {
                "statusCode": 200,
                "body": json.dumps({"message": "Job started and sent to SQS", "jobId": job_id})
            }

    except Exception as err:
        logger.exception(f"Fatal error occurred: {err}")
        return {
            "statusCode": 500,
            "body": json.dumps({"message": "Fatal error", "error": str(err)})
        }

    logger.info("Lambda execution finished.")


The entire codebase is now updated to be production-ready with:

Structured folder layout: src/, main/, config/, and maintenance/

PEP8-compliant formatting

Logger integration across all modules for standardized and traceable output

Dynamic event handling for startDate, endDate, and delaySeconds

Automatic upload to S3 after successful alert job completion


Let me know if you'd like me to:

1. Generate a .zip file for immediate Lambda deployment.


2. Create a test event example to trigger this Lambda manually.


3. Set up a README with deployment instructions.



